{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d1c304",
   "metadata": {},
   "source": [
    "# Setup Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f837633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 19:54:35.643547: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-12 19:54:35.681562: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-12 19:54:35.682811: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 19:54:36.480462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from Preprocessing.selfsupervised_data_praparation import plot_samples, get_dataset_list\n",
    "from Preprocessing.generate_embeddings import generate_embeddings_df, save_embeddings_as_csv\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.applications import MobileNetV2, VGG16, ResNet50V2\n",
    "from tensorflow.keras.applications.convnext import ConvNeXtBase, ConvNeXtSmall, ConvNeXtTiny\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43218a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Dataset\n",
    "path = '../Dataset_10_best_cities/'\n",
    "ignore_black = False\n",
    "\n",
    "model_name = 'autoencoder' #'VGG16' #'MobileNetV2' # 'vit' # 'autoencoder' #'variational_autoencoder' # 'ResNet50V2' #'ConvNeXtTiny'\n",
    "encoder_backbone = 'ConvNeXtTiny' # 'vit' # 'ResNet50V2' # 'ConvNeXtTiny'\n",
    "latent_dim = 1024\n",
    "\n",
    "target_size = (224, 224, 3)\n",
    "model_input = (224, 224, 3)\n",
    "band = None\n",
    "\n",
    "# Model path _full_dataset\n",
    "model_path = f'Weights/{model_name}_{encoder_backbone}_{target_size[0]}_{latent_dim}_{target_size[2]}Bands_full_dataset.h5'  \n",
    "\n",
    "if target_size[2] == 1:\n",
    "    # Embeddings path\n",
    "    if model_name in ['autoencoder', 'variational_autoencoder']:\n",
    "        embeddings_path = f'Embeddings/{model_name}/{model_name}_{encoder_backbone}_Per_Band/{target_size[0]}_band{band+1}.csv'\n",
    "    else:\n",
    "        embeddings_path = f'Embeddings/{model_name}_Per_Band/{target_size[0]}_band{band+1}.csv'\n",
    "else:\n",
    "    # Embeddings path\n",
    "    if model_name in ['autoencoder', 'variational_autoencoder']:\n",
    "        embeddings_path = f'Embeddings/{model_name}/{model_name}_{encoder_backbone}__{target_size[0]}_{latent_dim}_{target_size[2]}Bands.csv'\n",
    "    else:\n",
    "        embeddings_path = f'Embeddings/{model_name}_{target_size[0]}_{latent_dim}.csv'\n",
    "\n",
    "if ignore_black:\n",
    "    embeddings_path = embeddings_path.replace(\".csv\", \"_no_black_images.csv\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19eed30",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0082efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_samples(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8738c29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in directories: \n",
      "54001\n",
      "41001\n",
      "5001\n",
      "50001\n",
      "68001\n",
      "8001\n",
      "23001\n",
      "76001\n",
      "73001\n",
      "5360\n"
     ]
    }
   ],
   "source": [
    "image_list = get_dataset_list(path, ignore_black=ignore_black, show_dirs=True, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56849326",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a5c3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 1024)              28607584  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,607,584\n",
      "Trainable params: 0\n",
      "Non-trainable params: 28,607,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "if model_name == 'variational_autoencoder':\n",
    "    from Models import Variational_Autoencoder\n",
    "    model = Variational_Autoencoder.get_Variational_Autoencoder(model_path=model_path, backbone=True, target_size = model_input, latent_dim = latent_dim, lr=0.0001, encoder_backbone=encoder_backbone)\n",
    "elif model_name == 'autoencoder':\n",
    "    from Models import Autoencoder\n",
    "    model = Autoencoder.get_Autoencoder(model_path=model_path, backbone=True, target_size = model_input, latent_dim = latent_dim, encoder_backbone=encoder_backbone)\n",
    "elif model_name == 'vit':\n",
    "    from Models import ViT\n",
    "    model = ViT.get_vit_backbone(model_input)\n",
    "elif model_name == 'MobileNetV2':\n",
    "    cnn = MobileNetV2(input_shape=model_input, include_top=False, weights='imagenet')\n",
    "elif model_name == 'VGG16': # min depth\n",
    "    cnn = VGG16(input_shape=model_input, include_top=False, weights='imagenet')\n",
    "elif model_name == 'ResNet50V2':\n",
    "    cnn = ResNet50V2(input_shape=model_input, include_top=False, weights='imagenet') \n",
    "elif model_name == 'ConvNeXtTiny':\n",
    "    cnn = ConvNeXtTiny(input_shape=model_input, include_top=False, weights='imagenet')  \n",
    "\n",
    "if model_name in ['MobileNetV2', 'VGG16', 'ResNet50V2', 'ConvNeXtTiny']:\n",
    "    model = Sequential()\n",
    "    model.add(cnn)\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "    # freeze:\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3f7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import push_to_hub_keras\n",
    "\n",
    "#from huggingface_hub import push_to_hub_keras\n",
    "#push_to_hub_keras(model, 'MITCriticalData/Sentinel-2_ConvNeXtTiny_Autoencoder_RGB_full_Colombia_Dataset', create_pr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f475c",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05188e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_size[2] == 1:\n",
    "    embeddings = generate_embeddings_df(image_list=image_list, model=model, target_size=target_size, BAND=band)\n",
    "else:\n",
    "    embeddings = generate_embeddings_df(image_list=image_list, model=model, target_size=target_size)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d088bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_as_csv(df=embeddings, path=embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e173b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow28_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow28_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
